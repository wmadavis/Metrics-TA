---
title: "Problem Set 6 R Solutions"
author: Matthew Alampay Davis
date: April 13, 2021
output:
  pdf_document: default
---

```{r, include = FALSE}
setwd('~/Documents/Grad School/Columbia/Y2/Intro Metrics TA/Past problem sets/PS6')
library(readstata13)
library(dplyr)
library(magrittr)
library(estimatr)
library(lmtest)
library(sandwich)
library(margins)
library(pscl)
library(ggplot2)
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff=60), tidy=TRUE)
```

# Load the data

```{r}
amex <- read.dta13('AmEx.dta')
```

# a: Transformations and summary statistics

"select" is a function in *dplyr* whose arguments are just the variables we want to keep

```{r}
amex %<>% mutate(income = income/1000) %>%
  select(cardhldr, income, age, selfempl, ownrent, acadmos, majordrg, minordrg)
head(amex)
```

Now calculate the fraction of consumers who are cardholders. Several ways of doing this, here's two:

```{r}
mean(amex$cardhldr)

table(amex$cardhldr) %>% prop.table
```

78.28\% of consumers in the sample are cardholders

# b: Predicting cardholder status by regression

We can continue using *lm_robust* for the linear probability model since it's just OLS. For logit and probit, we must use the *glm* command that comes in-built with R stats. To get regression output that uses robust standard errors is a bit more involved:

```{r}
# Linear probability model
lpm.b <- lm_robust(cardhldr ~ income + age + selfempl + ownrent + acadmos,
                   data = amex)
summary(lpm.b)

# Logit
logit.b <- glm(cardhldr ~ income + age + selfempl + ownrent + acadmos,
               data = amex,
               family = binomial(link = 'logit'))
coeftest(logit.b, vcov. = vcovHC, type = 'HC1')

# Probit
probit.b <- glm(cardhldr ~ income + age + selfempl + ownrent + acadmos,
               data = amex,
               family = binomial(link = 'probit'))
coeftest(probit.b, vcov. = vcovHC, type = 'HC1')
```

# c: Marginal effects at median

```{r}
# Create an observation that's median for all covariates except selfselfempl = 1 and 0
# Create an observation that's median for all covariates except ownrent = 1 and 0
median.df <- summarize_all(amex, .funs = c('median'))
new.obs <- rbind(mutate(median.df, selfempl = 1),
                 mutate(median.df, selfempl = 0),
                 mutate(median.df, ownrent = 1),
                 mutate(median.df, ownrent = 0))
# Predicted probabilities
lpm.pred <- predict(lpm.b, newdata = new.obs)
logit.pred <- predict(logit.b, newdata = new.obs, type = 'response')
probit.pred <- predict(probit.b, newdata = new.obs, type = 'response')

# Differences
lpm.pred[1]-lpm.pred[2]
logit.pred[1]-logit.pred[2]
probit.pred[1]-probit.pred[2]

lpm.pred[3]-lpm.pred[4]
logit.pred[3]-logit.pred[4]
probit.pred[3]-probit.pred[4]

# Percentage differences
(lpm.pred[1]-lpm.pred[2])/lpm.pred[2]
(logit.pred[1]-logit.pred[2])/logit.pred[2]
(probit.pred[1]-probit.pred[2])/probit.pred[2]

(lpm.pred[3]-lpm.pred[4])/lpm.pred[4]
(logit.pred[3]-logit.pred[4])/logit.pred[4]
(probit.pred[3]-probit.pred[4])/probit.pred[4]
```


# d: Marginal effects at percentiles

Income at 20th percentile:

```{r}
# Create an observation that's median for all covariates except selfselfempl = 1 and 0, income = 20p
# Create an observation that's median for all covariates except ownrent = 1 and 0, income = 20p
median.df <- summarize_all(amex, .funs = c('median'))
new.obs <- rbind(mutate(median.df, selfempl = 1, income = quantile(amex$income, 0.2)),
                 mutate(median.df, selfempl = 0, income = quantile(amex$income, 0.2)),
                 mutate(median.df, ownrent = 1, income = quantile(amex$income, 0.2)),
                 mutate(median.df, ownrent = 0, income = quantile(amex$income, 0.2)))
# Predicted probabilities
lpm.pred <- predict(lpm.b, newdata = new.obs)
logit.pred <- predict(logit.b, newdata = new.obs, type = 'response')
probit.pred <- predict(probit.b, newdata = new.obs, type = 'response')

# Differences
lpm.pred[1]-lpm.pred[2]
logit.pred[1]-logit.pred[2]
probit.pred[1]-probit.pred[2]

lpm.pred[3]-lpm.pred[4]
logit.pred[3]-logit.pred[4]
probit.pred[3]-probit.pred[4]

# Percentage differences
(lpm.pred[1]-lpm.pred[2])/lpm.pred[2]
(logit.pred[1]-logit.pred[2])/logit.pred[2]
(probit.pred[1]-probit.pred[2])/probit.pred[2]

(lpm.pred[3]-lpm.pred[4])/lpm.pred[4]
(logit.pred[3]-logit.pred[4])/logit.pred[4]
(probit.pred[3]-probit.pred[4])/probit.pred[4]
```

Income at 80th percentile:

```{r}
# Create an observation that's median for all covariates except selfselfempl = 1 and 0, income = 20p
# Create an observation that's median for all covariates except ownrent = 1 and 0, income = 20p
median.df <- summarize_all(amex, .funs = c('median'))
new.obs <- rbind(mutate(median.df, selfempl = 1, income = quantile(amex$income, 0.8)),
                 mutate(median.df, selfempl = 0, income = quantile(amex$income, 0.8)),
                 mutate(median.df, ownrent = 1, income = quantile(amex$income, 0.8)),
                 mutate(median.df, ownrent = 0, income = quantile(amex$income, 0.8)))
# Predicted probabilities
lpm.pred <- predict(lpm.b, newdata = new.obs)
logit.pred <- predict(logit.b, newdata = new.obs, type = 'response')
probit.pred <- predict(probit.b, newdata = new.obs, type = 'response')

# Differences
lpm.pred[1]-lpm.pred[2]
logit.pred[1]-logit.pred[2]
probit.pred[1]-probit.pred[2]

lpm.pred[3]-lpm.pred[4]
logit.pred[3]-logit.pred[4]
probit.pred[3]-probit.pred[4]

# Percentage differences
(lpm.pred[1]-lpm.pred[2])/lpm.pred[2]
(logit.pred[1]-logit.pred[2])/logit.pred[2]
(probit.pred[1]-probit.pred[2])/probit.pred[2]

(lpm.pred[3]-lpm.pred[4])/lpm.pred[4]
(logit.pred[3]-logit.pred[4])/logit.pred[4]
(probit.pred[3]-probit.pred[4])/probit.pred[4]
```

# e: Express all three cases in terms of the proability ratio or log-odds. How big ist he impact? Is this a lot or a little?

The impact is pretty substantial around -15\% for being self-employed and about +7\% for owning your own home. It varies for the model and is larger at the median for the nonlinear models than at the more extreme values.

# f: Consider a specification that includes major and minor derogatory credit events in the last year (*majdrg*, *mindrg*). Write commands for LPM, logit and probit.

```{r}
# Linear probability model
lpm.f <- lm_robust(cardhldr ~ income + age + selfempl + ownrent + acadmos + minordrg + majordrg,
                   data = amex)
summary(lpm.f)

# Logit
logit.f <- glm(cardhldr ~ income + age + selfempl + ownrent + acadmos + minordrg + majordrg,
               data = amex,
               family = binomial(link = 'logit'))
coeftest(logit.f, vcov. = vcovHC, type = 'HC1')

# Probit
probit.f <- glm(cardhldr ~ income + age + selfempl + ownrent + acadmos + minordrg + majordrg,
               data = amex,
               family = binomial(link = 'probit'))
coeftest(probit.f, vcov. = vcovHC, type = 'HC1')
```

# g: Just see the regression outputs above

To get pseudo-R2 for probit orlogit models, we'll need the *pscl* package:

```{r}
pR2(logit.f)
```

Alternatively,

```{r}
1-(logit.f$deviance)/(logit.f$null.deviance)
```

The statistic we want is the McFadden statistic here

# h: Which model fits the data the best? Which variables have the most explanatory power? Do major derogatory credit events matter? Do you prefer models with or without derogatory events?

Probably model 5, the logit including the credit events. Everything but age appears to be significant.Major derogatory credit events seem to be the key explanatory variable both increasing the fit and having a substantial impact on outcomes.

# i: Construct a 95% confidence interval for the effect of an additional major derogatory credit event has on being an American Express cardholder under the linear probability model.

For the LPM the marginal effect is just the confidence interval for $\beta_{majordrg}$:

```{r}
confint(lpm.f)
```

# j: Repeat for the logit and probit models in the case where the x values take on the median values in the sample.

We'll need the *margins* package. And recall we've already defined a data.frame that takes on median values for each covariate. The margins command requires us to convert this into list form so that's what we use for the second argument.

```{r}
logit.se <- vcovHC(logit.f, type = 'HC1')
probit.se <- vcovHC(probit.f, type = 'HC1')
margins(logit.f,
        at = as.list.data.frame(median.df),
        vcov = logit.se) %>%
  summary
margins(probit.f,
        at = as.list.data.frame(median.df),
        vcov = probit.se) %>%
  summary
```

These give us exactly the Stata confidence intervals (lower and upper)

# k: Compute the fitted values for the median values of the linear probability model, and plot a histogram for the fitted values of the y variable. Do the same for logit and probit models.

```{r}
lpm.fit <- data.frame(y.hat = lpm.f$fitted.values)
probit.fit <- data.frame(y.hat = probit.f$fitted.values)
logit.fit <- data.frame(y.hat = logit.f$fitted.values)

ggplot(lpm.fit, aes(x = y.hat)) +
  theme_minimal() +
  ggtitle('Distribution of fitted values (LPM)') +
  ylab('') + xlab('Fitted values') +
  geom_histogram()

ggplot(probit.fit, aes(x = y.hat)) +
  theme_minimal() +
  ggtitle('Distribution of fitted values (Probit)') +
  ylab('') + xlab('Fitted values') +
  geom_histogram()

ggplot(logit.fit, aes(x = y.hat)) +
  theme_minimal() +
  ggtitle('Distribution of fitted values (Logit)') +
  ylab('') + xlab('Fitted values') +
  geom_histogram()
```


# l: What should be the domain of predicted values from the LPM? What fraction of values lies outside this domain? How do we interpret them?

The domain should be between 0 and 1.

```{r}
table(lpm.fit$y.hat < 0)
table(lpm.fit$y.hat > 1)
```

147/12604 are below zero and 363/12604 are above 1. We cannot interpret these values as probabilities. This indicates that the LPM is mis-specified, or that it is not the right tool for the job.

# m: From the results you find on Table 1, what can you say about probit and logit results? How about LPM results?

The relationship between logit and probit looks okay but not the relationship to the LPM (the LPM is about half as big as it needs to be).

# n: A well-known result by Amemiya (1981) is that over the range of probabilities from 30\% to 70\% the following approximations should hold: $\beta_{LPM} \approx 0.4\beta_{probit}$ and $\beta_{LPM} \approx 0.25\beta_{logit}$. Take the median results for logit and probit, how does this result look for the $\beta_{majordrg}$?

They do not match.