---
title: "PS3 R Solutions"
author: "Matthew Alampay Davis"
date: "February 9, 2021"
output:
  pdf_document: default
  html_notebook: default
---

```{r, include = FALSE}
setwd('~/Documents/Grad School/Columbia/Y2/Intro Metrics TA/Recitation 4')
library(readstata13)
library(dplyr)
library(estimatr)
library(ggplot2)
library(magrittr)
library(car)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

## Question 1 ##

```{r}
gpa4 <- read.dta13('GPA4.dta')
gpa.mod1 <- lm_robust(colGPA ~ hsGPA + skipped, data = gpa4, se_type = 'stata')
gpa.mod2 <- lm_robust(colGPA ~ hsGPA + skipped + PC, data = gpa4, se_type = 'stata')
gpa.mod3 <- lm_robust(colGPA ~ hsGPA + skipped + PC + bgfriend + campus, data = gpa4, se_type = 'stata')
```

### Part a

```{r}
summary(gpa.mod1)
```

$$
\begin{aligned}
\hat{\text{colGPA}} = &\ 1.579 + &\ 0.458 \text{hsGPA} &\ -0.077\text{skipped}\\
                      &\ (0.325) &\ (0.094)            &\ (0.025)
\end{aligned}
$$

### Part b

The coefficient on hsGPA is 0.458. This means that a one point increase in hsGPA will lead to an increase of 0.458 points in colGPA (college GPA). Students with higher GPAs in high school tend to have higher college GPAs.

### Part c

The t-statistic for H0: skipped=0 vs H1: skipped!= 0 is -3.05. Since |-3.05|>1.96 we can reject H0 and conclude skipped is statistically different than 0. We are essentially testing if on average skipping classes would affect studentsâ€™ college GPA (in layman words).

### Part d

```{r}
gpa.mod1$statistic['skipped']
gpa.mod2$statistic['skipped']
gpa.mod3$statistic['skipped']
```

The critical value for a two-sided test at the 1% significance level is 2.58, so we reject H0 (that the coefficient on skipped is equal to zero) in Regressions 1 and 3. We cannot reject H0 in regression 2. (We may also compare the p-values with 0.01 as instructed in the problem)

### Part e

```{r}
gpa.mod3$coefficients['campus']
```

The coefficient on campus is -0.124. This means living on campus reduces colGPA by 0.124 points. The negative sign on the coefficient might be because students who live on campus have more distractions that they would if they lived at home. However, students who live on campus are also more able to study with one another, so it is not clear whether the sign of the coefficient should be positive or negative; it would depend on which effect was stronger. The size of the coefficient is about one-tenth of a point, which is small. Note that the coefficient is not statistically different than 0, so there is not too much we can say.

### Part f

```{r}
gpa.mod3$coefficients['bgfriend']
```

The coefficient on bgfriend is 0.085. This means having a boyfriend or girlfriend increases GPA by 0.085. One would expect the coefficient to be positive or negative, such as: the coefficient could be positive if students who are more intelligent are more likely to be dating (here we have a correlation), alternatively, the sign could be negative if dating distracts a student from studying. However, here the magnitude of the coefficient is very small also note that the coefficient is not statistically significant. Hence, we can conclude that there is no significant effect on GPA of dating.

## Question 2

```{r}
nurse <- read.dta13('WiscoNursingHome.dta') %>%
  mutate(logtpy = log(tpy),
         lognumbed = log(numbed),
         logsqrfoot = log(sqrfoot))
nurse.2000 <- filter(nurse, cryear == 2000)
nurse.2001 <- filter(nurse, cryear == 2001)
```

### Question 2.1

#### Part a)
```{r}
# i)
cor(nurse.2000$tpy, nurse.2000$logtpy)

# ii)
nurse.2000[,c('tpy', 'logtpy', 'sqrfoot')] %>% na.omit() %>% cor
```

The correlation between TOY and LOG(TPY) is very strong and close to 1 (i.e., 94%). The correlation coefficients among these three variables appear to be highly correlated. The lowest is 82 % and the highest is 97% correlation.

#### Part b)

```{r}
ggplot(nurse.2000, aes(x = numbed, y = tpy)) +
  theme_bw() +
  geom_point()
```

Based on  visual inspection of this plot there is evidence for positive correlation between
these two variables and the association seems to be precise.

```{r}
ggplot(nurse.2000, aes(x = sqrfoot, y = tpy)) +
  theme_bw() +
  geom_point()
```

Based on visual inspection of this plot like the previous one there is evidence for positive correlation between these two variables but the association seems to be less precise

#### Part c)

```{r}
nurse.mod1 <- lm_robust(tpy ~ numbed, data = nurse.2000)
nurse.mod2 <- lm_robust(tpy ~ sqrfoot, data = nurse.2000)
nurse.mod3 <- lm_robust(tpy ~ lognumbed, data = nurse.2000)
nurse.mod4 <- lm_robust(tpy ~ logsqrfoot, data = nurse.2000)

nurse.mod1$r.squared
nurse.mod1$statistic[2]
nurse.mod2$r.squared
nurse.mod2$statistic[2]
nurse.mod3$r.squared
nurse.mod3$statistic[2]
nurse.mod4$r.squared
nurse.mod4$statistic[2]
```

### Question 2.2

Same thing just replace the filter with 2001

## Question 3

```{r}
wage <- read.dta13('WAGE1.dta')
```

### Part a

For the omitted variable $X_2$ to cause omitted variable bias (OVB), it should satisfy the following to conditions:

i) Years of potential experience $X_2$ should be a determinant factor for average hourly earnings/wage $Y$). That is, $Y=f(X_2)$ so $X_2$ is part of the error term $u$.

ii) Years of potential experience $X_2$ should be correlated with years of education $X_1$). Mathematically, it means that their correlation is non-zero2) != 0. Intuitively, this implies that more years of experience is correlated or sometimes affects years of education. The more you spent your years in acquiring work experience, the less time you are left with to spend in (formal) education or the number of years of education would increase as you might be taking few courses (i.e., you are part time student) as you are on the job.

Math is in the official solutions

### Part b

```{r}
wage.mod <- lm_robust(wage ~ female, data = wage, se_type = 'stata')
summary(wage.mod)
```

Since $X_2$ = gender dummy (binary) variable that takes the value of 1 if female and 0 otherwise, the slope coefficient is interpreted as the difference-in-group mean. That is, average hourly earnings declines by \$2.51 if the individual is female. Mathematically,

$$
\beta_4 = E[Y_i|X_4 = 1] - E[Y_i|X-4=0] = -2.512
$$

### Part c

```{r}
wage %<>% mutate(D = 1-female)
```

Since female = 0 are male individuals, this generate command would give you D = 1 for male and D = 0 for female. In other words, D and female are dummy variables that takes opposite values. D = 1 is the same as female = 0.

```{r}
wage.mod2 <- lm_robust(wage ~ educ + female + D, se_type = 'stata', data = wage)
summary(wage.mod2)
```

The error tells us there is multicollinearity. In Stata, this is displayed by having the variable D drop out of the regression. Here, it removes the intercept. Both are ways of removing multicollinearity.

See math in the official solutions.

### Part d

```{r}
wage.mod3 <- lm_robust(wage ~ educ, data = wage, se_type = 'stata')
summary(wage.mod3)

wage.mod4 <- lm_robust(wage ~ educ + exper, data = wage, se_type = 'stata')
summary(wage.mod4)
```

As can be seen from the above two tables, the coefficient on education has increased from 0.54 to 0.64. The reason for this increment is the addition of one of the omitted variable, namely, experience. The fact that it is also statistically significant suggests that it is one of the determinant variable for our dependent variable (condition #1). This result is similar to the test score example that we are using in the text that when we add percentage of English language learner in the model, the coefficient on class size has changed.

### Part e

```{r}
wage.mod5 <- lm(wage ~ educ + exper + tenure + female + nonwhite, data = wage)
wage.mod6 <- lm_robust(wage ~ educ + exper + tenure + female + nonwhite, data = wage, se_type = 'stata')

summary(wage.mod5)
summary(wage.mod5)
```

Here the first table provides a regression result based on homoscedasticity-only standard error and the second one is based on heteroscedasticity-robust standard errors. As it can be seen from these two tables, the coefficients are the same in both cases but the corresponding standard errors are different for each coefficient. Since, the remaining t- statistics, p-values, and the resulting confidence intervals in the two tables are different as all of them are dependent of the standard errors. The interpretation will proceed as usual.
We care about the presence of heteroscedasticity in the data because, if indeed there is the problem of heteroscedasticity, the homoscedasticity-only standard errors will be wrong. As mentioned above, if the standard errors are wrong, then everything else that depends on these wrong standard errors will result in misleading and incorrect statistical inference. It is advisable to use heteroscedasticity-robust standard errors whenever possible even if there is no heteroscedasticity. This is because, if there is no heteroscedasticity in the data, both will give us the correct standard errors. (see page 163 of the text on this issue.)

### Part f

i) Testing the null hypothesis for single coefficients being equal to zero

For individual null hypothesis of these coefficients, yo u can directly use the reported t-statistics and the corresponding p-values and confidence intervals.

```{r}
wage.mod6$statistic
wage.mod6$p.value
confint(wage.mod6)
```

ii) Testing joint hypotheses

```{r}
linearHypothesis(wage.mod6, c('educ = 0', 'exper = 0', 'tenure = 0', 'female = 0', 'nonwhite = 0'), test = 'F')
# Note we can also do the heteroskedasticity-robust test using the non-robust model wage.mod5
linearHypothesis(wage.mod5, c('educ = 0', 'exper = 0', 'tenure = 0', 'female = 0', 'nonwhite = 0'), white.adjust = 'hc1')
```

As you can see, the computed F-stat is 35.62 and from the F-distribution table we know that the 1%, 5%, and 10% critical values for q =5 are 3.02, 2.21 and 1.85, respectively. This implies that we can reject the null hypothesis of all slope coefficients are zero. In fact, STATA has already computed the p-value for you and it is Prob > F = 0.0000. This implies that we can reject the null at 1% significance level.
