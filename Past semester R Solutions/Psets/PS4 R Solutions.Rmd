---
title: "PS4 R Solutions"
output:
  pdf_document: default
  html_notebook: default
---

```{r, include = FALSE}
setwd('~/Documents/Grad School/Columbia/Y2/Intro Metrics TA/Recitation 5')
library(readstata13)
library(dplyr)
library(estimatr)
library(ggplot2)
library(magrittr)
library(car)
library(lmtest)
```

# Question 1

Loading the data

```{r}
hprice1 <- read.dta13('hprice1.dta')
head(hprice1)
```

## Part a ##

```{r}
q1.mod <- lm_robust(price ~ sqrft + bdrms, hprice1)
summary(q1.mod)
```

$$
\hat{\text{Price}} = -19.32 + 0.13 \text{sqrft} + 15.20 \text{bdrms}
$$

## Part b ##

Holding square footage constant, and so price increases by 15.20 for each additional bedroom. Since the unit of price is in thousands this means, \$15,200.

## Part c ##

$$
\Delta \hat{\text{Price}} = 0.13 \times 1400 + 15.20 \times 1
$$

```{r}
0.128*1400+15.20*1
```

Since unit of price is in thousands this means \$194,400 Because the houseâ€™s size is increasing as well, the total effect is much larger in (c). In part (b) the additional bedroom is obtained by converting existing rooms in the house so square footage remains unchanged. In (c), the added bedroom increases the square footage so the effect on price is much larger.

## Part d ##

```{r}
q1.mod$r.squared
```

So about 63.19\%. On the other hand, adjusted $R^2 = 0.623$, which is smaller. By construction, adjusted $R^2$ is always smaller than R2; this is due to the fact that it takes into account the presence of k = 2 regressors in the equation.

## Part e ##

```{r}
hprice1[1,]
```

We see that sqrft = 2,438 and bdrms = 4. The predicted price is then

$$
\hat{\text{price}} = -19.32 + 0.128 \times 2,438 + 15.20 \times 4
$$
```{r}
-19.32 + 0.128*2438+15.2*4
```

The unit of price is in thousands, so \$353,544. Thus, we expect the house to be worth \$353,544.

## Part f ##

```{r}
hprice1$price[1]
```

Finding the residual for this house:

```{r}
hprice1$price[1]-q1.mod$fitted.values[1]
```

This could suggest that the buyer underpaid by some margin. However, there are many other features of a house (some that we cannot even measure) that affect price, and we have not controlled for these. Thus, the negative residual could simply be a consequence of those other features made the house less attractive/valuable.

# Question 2

## Load the data

```{r}
cps <- read.dta13('cps92_12.dta')
head(cps)
summary(cps)
```

## Part a

Creating log transformed and interaction variables

```{r}
cps %<>% mutate(lahe = log(ahe),
                femxbac = female*bachelor)

```

Regression 1:

```{r}
# Method 1
reg1 <- lm_robust(lahe ~ age + female + bachelor + femxbac, cps)
summary(reg1)

# Method 2
reg1 <- lm_robust(lahe ~ age + female + bachelor + female:bachelor, cps)
summary(reg1)
```

## Part b

Regression 2:

```{r}
reg2 <- lm_robust(lahe ~ female + age + bachelor + female:age, cps)
summary(reg2)
```

## Part c

Use Regression 1:

```{r}
f.bach <- data.frame(age = 3, female = 1, bachelor = 1)
m.bach <- data.frame(age = 3, female = 0, bachelor = 1)
predict(reg1, newdata = f.bach)-predict(reg1, newdata = m.bach)
```

Females with bachelor degree are expected to earn about 12.28\% less than males with bachelor degree keeping age unchanged.

## Part d

Use Regression 1:

```{r}
f.bach <- data.frame(age = 3, female = 1, bachelor = 1)
f.nobach <- data.frame(age = 3, female = 1, bachelor = 0)
predict(reg1, newdata = f.bach)-predict(reg1, newdata = f.nobach)
```

Females with a bachelor degree are expected to earn about 54.43\% more than females without.

## Part e

Null hypothesis:

$$
H_0: \beta_{\text{Female}} + \beta_{\text{Female}\times\text{Bachelor}} = 0
$$

Method 1: Linear hypothesis

```{r}
linearHypothesis(reg1, c('female = 0', 'female:bachelor = 0'), test = 'F')
```

Method 2: Fooling Stata/R

```{r}
cps %<>% mutate(new = female*bachelor-female)
q2.mod2 <- lm_robust(lahe ~ age + female + bachelor + new, cps)
# Then check the coefficient on female
summary(q2.mod2)
```

Either method tells us to reject the null hypothesis

## Part f

We have to use Regression 2.

To test the intercept difference,

$$H_0: \beta_{\text{Female}} = 0$$

To test the slope difference,

$$H_0: \beta_{\text{Female}\times\text{Age}}= 0$$

```{r}
summary(reg2)
```

We reject both null hypotheses

## Part g

Draw the graph elsewhere.

Must use regression 2. The female regression line must start from a hgiher point and the gap narrows

## Part h

You have to calculate the test statistic yourself, I think, but to test on R:

```{r}
coeftest(reg2)
```

The p-value is only 0.08 so it is not significant at the 1\% significance level